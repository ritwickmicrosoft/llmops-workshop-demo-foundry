# =============================================================================
# LLMOps Evaluation Pipeline
# =============================================================================
# Runs quality evaluations when prompts or RAG logic changes
# Ensures model quality gates are passed before deployment
# =============================================================================

name: LLMOps - Evaluation

on:
  pull_request:
    branches: [main, master]
    paths:
      - '01-rag-chatbot/**'
      - '02-evaluation/**'
      - '04-frontend/app.py'  # Contains system prompt
      - 'data/**'
  workflow_dispatch:
    inputs:
      samples:
        description: 'Number of samples to evaluate'
        required: false
        default: '10'
        type: string
      threshold:
        description: 'Minimum score threshold (1-5)'
        required: false
        default: '3.0'
        type: string

env:
  PYTHON_VERSION: '3.11'
  # Quality thresholds
  MIN_GROUNDEDNESS: 3.0
  MIN_FLUENCY: 3.0
  
jobs:
  # ===========================================================================
  # Run Evaluation
  # ===========================================================================
  evaluate:
    name: üìä Run Quality Evaluation
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write
    env:
      AZURE_OPENAI_ENDPOINT: ${{ secrets.AZURE_OPENAI_ENDPOINT }}
      AZURE_OPENAI_DEPLOYMENT: ${{ secrets.AZURE_OPENAI_DEPLOYMENT || 'gpt-4o' }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install azure-ai-evaluation pandas

      - name: Azure Login
        uses: azure/login@v2
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      - name: Get Azure token
        id: token
        run: |
          TOKEN=$(az account get-access-token --resource https://cognitiveservices.azure.com --query accessToken -o tsv)
          echo "::add-mask::$TOKEN"
          echo "token=$TOKEN" >> $GITHUB_OUTPUT

      - name: Run Evaluation
        id: eval
        env:
          AZURE_AD_TOKEN: ${{ steps.token.outputs.token }}
          MAX_SAMPLES: ${{ github.event.inputs.samples || '10' }}
        run: |
          cd 02-evaluation
          python run_evaluation.py 2>&1 | tee eval_output.txt
          
          # Extract scores from output
          GROUNDEDNESS=$(grep -oP 'Groundedness:.*?(\d+\.?\d*)' eval_output.txt | tail -1 | grep -oP '\d+\.?\d*' || echo "0")
          FLUENCY=$(grep -oP 'Fluency:.*?(\d+\.?\d*)' eval_output.txt | tail -1 | grep -oP '\d+\.?\d*' || echo "0")
          
          echo "groundedness=$GROUNDEDNESS" >> $GITHUB_OUTPUT
          echo "fluency=$FLUENCY" >> $GITHUB_OUTPUT

      - name: Upload evaluation results
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-results
          path: |
            02-evaluation/eval_results/
          retention-days: 30

      - name: Check Quality Gates
        id: gates
        run: |
          GROUNDEDNESS="${{ steps.eval.outputs.groundedness }}"
          FLUENCY="${{ steps.eval.outputs.fluency }}"
          THRESHOLD="${{ github.event.inputs.threshold || '3.0' }}"
          
          echo "### üìä Evaluation Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Score | Threshold | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|-----------|--------|" >> $GITHUB_STEP_SUMMARY
          
          PASSED=true
          
          # Check Groundedness
          if (( $(echo "$GROUNDEDNESS >= $THRESHOLD" | bc -l) )); then
            echo "| Groundedness | $GROUNDEDNESS | $THRESHOLD | ‚úÖ |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| Groundedness | $GROUNDEDNESS | $THRESHOLD | ‚ùå |" >> $GITHUB_STEP_SUMMARY
            PASSED=false
          fi
          
          # Check Fluency
          if (( $(echo "$FLUENCY >= $THRESHOLD" | bc -l) )); then
            echo "| Fluency | $FLUENCY | $THRESHOLD | ‚úÖ |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| Fluency | $FLUENCY | $THRESHOLD | ‚ùå |" >> $GITHUB_STEP_SUMMARY
            PASSED=false
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "$PASSED" = true ]; then
            echo "### ‚úÖ All quality gates passed!" >> $GITHUB_STEP_SUMMARY
            echo "passed=true" >> $GITHUB_OUTPUT
          else
            echo "### ‚ùå Quality gates failed - review required" >> $GITHUB_STEP_SUMMARY
            echo "passed=false" >> $GITHUB_OUTPUT
          fi

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const groundedness = '${{ steps.eval.outputs.groundedness }}';
            const fluency = '${{ steps.eval.outputs.fluency }}';
            const passed = '${{ steps.gates.outputs.passed }}' === 'true';
            const threshold = '${{ github.event.inputs.threshold || '3.0' }}';
            
            const status = passed ? '‚úÖ Passed' : '‚ùå Failed';
            const emoji = passed ? 'üéâ' : '‚ö†Ô∏è';
            
            const body = `## ${emoji} LLMOps Evaluation Results
            
            | Metric | Score | Threshold | Status |
            |--------|-------|-----------|--------|
            | Groundedness | ${groundedness} | ${threshold} | ${parseFloat(groundedness) >= parseFloat(threshold) ? '‚úÖ' : '‚ùå'} |
            | Fluency | ${fluency} | ${threshold} | ${parseFloat(fluency) >= parseFloat(threshold) ? '‚úÖ' : '‚ùå'} |
            
            ### Overall: ${status}
            
            <details>
            <summary>üìñ Score Interpretation</summary>
            
            | Score | Meaning |
            |-------|---------|
            | 5 | Excellent - Perfect quality |
            | 4 | Good - Minor issues |
            | 3 | Acceptable - Some issues |
            | 2 | Poor - Significant issues |
            | 1 | Bad - Major problems |
            
            </details>
            
            ---
            *Evaluated with Azure AI Evaluation SDK*`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });

      - name: Fail if quality gates not met
        if: steps.gates.outputs.passed == 'false'
        run: |
          echo "‚ùå Quality gates failed - blocking merge"
          exit 1

  # ===========================================================================
  # Content Safety Testing
  # ===========================================================================
  content-safety:
    name: üõ°Ô∏è Content Safety Tests
    runs-on: ubuntu-latest
    needs: evaluate
    env:
      AZURE_OPENAI_ENDPOINT: ${{ secrets.AZURE_OPENAI_ENDPOINT }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Azure Login
        uses: azure/login@v2
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      - name: Get Azure token
        id: token
        run: |
          TOKEN=$(az account get-access-token --resource https://cognitiveservices.azure.com --query accessToken -o tsv)
          echo "::add-mask::$TOKEN"
          echo "token=$TOKEN" >> $GITHUB_OUTPUT

      - name: Run Content Safety Tests
        env:
          AZURE_AD_TOKEN: ${{ steps.token.outputs.token }}
        run: |
          cd 03-content-safety
          python test_content_safety.py 2>&1 | tee safety_output.txt

      - name: Upload safety results
        uses: actions/upload-artifact@v4
        with:
          name: content-safety-results
          path: |
            03-content-safety/test_results/
          retention-days: 30

      - name: Check for failures
        run: |
          if grep -q "FAILED" 03-content-safety/test_results/*.json 2>/dev/null; then
            echo "‚ö†Ô∏è Some content safety tests reported failures"
            echo "Review the test results for details"
          else
            echo "‚úÖ Content safety tests completed"
          fi

  # ===========================================================================
  # Evaluation Summary
  # ===========================================================================
  summary:
    name: üìã Summary
    runs-on: ubuntu-latest
    needs: [evaluate, content-safety]
    if: always()
    steps:
      - name: Generate Summary
        run: |
          echo "## ü§ñ LLMOps Pipeline Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Quality Evaluation | ${{ needs.evaluate.result == 'success' && '‚úÖ Passed' || '‚ùå Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Content Safety | ${{ needs.content-safety.result == 'success' && '‚úÖ Passed' || '‚ö†Ô∏è Review' }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ needs.evaluate.result }}" = "success" ]; then
            echo "### ‚úÖ Ready for deployment" >> $GITHUB_STEP_SUMMARY
          else
            echo "### ‚ùå Requires attention before deployment" >> $GITHUB_STEP_SUMMARY
          fi
